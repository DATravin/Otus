Я скопировал не все файлы, а 3 (для экономии времени)

Файлы на s3 и на hdfs

Скрипт для очистки данных лежит на прокси и мастер ноде

Для вызова небходимо запустить python3 cleaning_data.py -fn '2022-11-04.txt'
передав в качестве параметра названия файла на hdfs

![image](https://github.com/user-attachments/assets/4a6aff37-17d2-4125-aa67-93a9f73821bf)




Логика обработки:
1) парсим данные из txt
2) далее удаляем выброс tx_amount с помощью 1% и 99% перцентилей (откидываем 2% данных снизу и сверху)
3) далее удаляем всякие "мутные" значения из полей, где не должно быть текста. Это всякие записи типа "error"
4) удаляем данные, где есть пропущенне значения
5) форматируем все столбцы, приводя их к правильным типам данных
6) записываем в ввиде паркетов с названием типа /user/ubuntu/data/20221104.parquet


![image](https://github.com/user-attachments/assets/a03997e0-0784-4e1c-94cb-84fa687168af)

Далее еще один скрипт upload_data_from_hdfs перекладывает файлы обратно в S3
