




# План работы

Комментарий: я пока не понял, как отдельно создать S3, чтобы не убивать хранилище при разборке кластера. Ищу примеры реп, где это делает. Пока не нашел.
Потому как копировать 120 GB каждый раз - это очень долго. Но при этом хранить их постоянно...это же тоже будет кушать деньги. Очень хочется посмотреть реализацию репы с разбивкой по модулям. Самостоятельно пока не получается сделать все, как надо:( 


## Создать бакет S3

Бакет создается в общем процессе внутри main.tf
bucket_name = "otus-bucket-b1gcs058gd4fa8eeviec"
Он публичный:

![image](https://github.com/user-attachments/assets/6a7b20fe-531f-4b4d-8bbb-190dc63ebce5)

## скопировать данные 

Данные копируются внутри скрипта user_date.sh

![image](https://github.com/user-attachments/assets/556eadb2-b59b-4ebf-8c0e-766c4774c842)

Результат:

![image](https://github.com/user-attachments/assets/21ec9da3-7538-4663-9b3b-d7275b3ab5e2)


## поднять Spark-cluster

![image](https://github.com/user-attachments/assets/03b689fe-9ea6-4fbd-8587-d143265c6e47)

Поднят. Но пока с ограниченным ресурсом (40 GB на хост)

Тут параметры:

![image](https://github.com/user-attachments/assets/a5a09787-2991-4710-9cbb-7f0d94520496)


## подрубится к мастер-ноде. И скопировать из S3 в HDFS

Сделано. Вот результаты:


## Предложить способы для оптимизации затрат

Ничего кроме прерываемых VM не знаю:(

## занести в канбан

https://github.com/users/DATravin/projects/3/views/1 
добавил 2 таски ДО основного пайплана. Загрузить данные на hdfs с s3



