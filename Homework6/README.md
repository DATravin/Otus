# Архетиктурные особенности решения

было поднято 2 Виртуальных машины:

1) первая под Airflow
2) вторая под Mlflow

Так же в решение участвуют 2 бакета. 
1) разварачивается через terraform. Его функция: хранение артефактов Mlflow (именно он используется в качестве backend'а) + туда загружается .py скрипт из репы
2) Второй стационарный "cold". Его функциональность: хранение архива с окружением, который я делал отдельно. А также хранение сохраненных витрин с боевыми данными.

# организация данных

Процесс очистки данных из предыдущих этапов работы занимал довольно продолжительное время. Поэтому у меня на отработано 3 месяца (3 файла), сложенны в виде паркетов на холодное хранилище.




Так же для предсказания "плохих" транзакций у меня заранее на базе основных данных расчитаны 2 дополнительные витрины, с фичами по customer_id и terminal_id
В процессе обучения / инференса происходит соединение 3-х витрин по нужным ключам.

ниже хранение 3-х предагрегатов для скоринга с дневными патрициями. 

![image](https://github.com/user-attachments/assets/fcf440ee-1519-4a58-90f3-c1b00141937f)

(предполагается, что наша модель работает в оффлайне раз в сутки. Поэтому сперва считаются витрины за прошлый день, а после уже происходит скоринг текущего дня)

Для нескольких итераций обучения берется несколько дней в качестве train и один день для test.
Очень важно соблюсти временные рамки. Простой train-test-split здесь не применими и приведет к логическим ошибкам и драматической разнице между качеством на инференсе и при обучении

так как фичей в принциипе не много, данный объем более чем избыточен для модели.

Для того, чтобы интерации отличались, тестовая выборка всегда одинакова. А треновая меняется (изначально она одна, но мы берем случайны sample = 0.5 из нее)

# Mlflow

В качестве backend используется кластер Postgree 
В коде есть также модуль для поднятия Mysql. Но с ним запустить не вышло. Но код я оставил на всякий случай.

При создании ВМ мы также:
  накатываем все необходимые библиотеки
  загружаем сертификат для postgree
  объявляем все необходимые переменные
  создаем конфиг для s3
Таким образом после создания остается только в скрине запустить одну команду:

mlflow server --backend-store-uri postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${DB_NAME}?sslmode=verify-full --default-artifact-root s3://${S3_BUCKET}/artifacts -h 0.0.0.0 -p 8000

Вся логика сборки VM в Mlflow:
https://github.com/DATravin/otus-hw6-mlflow/blob/stage-0.0.1/infra/modules/mlflow/main.tf

И .sh файлик со всеми подготовительными шагами для VM:
https://github.com/DATravin/otus-hw6-mlflow/blob/stage-0.0.1/infra/modules/mlflow/scripts/setup.sh 


# Airflow

Обучение модели реализовано в ввиде дага. Даг лежит тут:
https://github.com/DATravin/otus-hw6-mlflow/blob/stage-0.0.1/dags/model_fitting.py  

Окружение сохраненов отдельно. И лежит на холодном хранилище.

# Переобучение и оптимизация модели

Сам скрипт переобучения модели тут:
https://github.com/DATravin/otus-hw6-mlflow/blob/stage-0.0.1/src/model_fit.py 

Для оптимизации модели используется библиотека hyperopt

Модель - Randomforest.
Перебираем 2 параметра: 
  число деревьев
  глубина дерева
В рамках одного запуска 5 итераций перебора.
Всего в примере 2 успешных запуска (10 экспериментов)

Ниже все эксперименты:

![image](https://github.com/user-attachments/assets/ad3fb96a-077b-4dbe-b025-afb37d53b619)

Для примера один:

![image](https://github.com/user-attachments/assets/f42313e4-98ac-4b6a-a31c-d2c3cb1916e1)

Слева параметры. Справа метрики. Оптимизация идет по rocauc
Но так же я расчитал другие метрики классификации для trashhold = 0.2 (0.5 не подошел, но это я делал отдельный Rnd)
Тут стандартный accurecy, recall, precision. А также f1 и модернизированный f-score, где recall'у дается вес 1,5. В данном случае нам важнее получить повышенный охват, нежели точность.

В Airflow это выглядит так:

![image](https://github.com/user-attachments/assets/b0d398a6-bdaf-4376-a4e0-6716995f5d0c)

В данном случае 2 полноценных запуска.
  



