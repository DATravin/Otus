
Выполнение ДЗ:


на базе предыдущих ДЗ я сделал процедуру "inference"

Его специфика в том, что мы не поднимаем м ходим в ML-flow, а работает уже с артефактом, который был до этого создан ml-flow
и лежит на s3 (модель)

далее был собран контейнер, содержащий:
-app
-inference.py
-Dockerfile
-requarments

особенность контейнера в том, что он довольно тяжелый. Для инференса "близкого к проду" нам нужно затащить туда спарк, млфлоу, панду, jvm
на легкую основу установить не получилось. Не ставилась jvm
Пришлось ставить на ubuntu 22.

После контейнер был загружена в репозиторий на docker.hub

После этого на отдельной VM был развернуть kind кластер
подняты сервис, ингресс и деплоймент с указанием на контейнер с приложением.

Кластер развернут. 4 ноды:
![image](https://github.com/user-attachments/assets/0f336e43-ae23-404b-885a-2e50c2ac5395)

поды
![image](https://github.com/user-attachments/assets/112cf276-1845-43a7-9037-a6c5b79084ee)


Тестовый режим работы приложения описан в ручке /predict
он моделирует тестовый запрос к модели, куда кидает семпл данных.
Скачивает модель из s3, осуществлеяет инференс и возвращает скоринг данного значения.

В итоге дергаем ручку и получаем результат:

![image](https://github.com/user-attachments/assets/ac21c356-591d-49c0-b27a-c9e4542018b0)

это означает, что инференс штатно сработал






