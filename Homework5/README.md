# Общее описание

Общая логика архитектуры повторяет общие контуры архитектры примера с семинара
Отличие в том, что у нас другой источник данных

наш источник - это наш публичный s3 otus-mlops-source-data
Оттуда мы забираем данные в наш текущий бакет. Кладем в папку input_data
результат отработки скрипта по очистке будет складываться также в бакет в папку output_data в виде партицированного паркета

Оттуда я буду уже забирать результат на свой s3, который не будет разобран после terraform destroy

# загрузка данных

Для демонстрации смоделируем ситуацию.

При первом запуске у нас будет 2 файла с данными:

![image](https://github.com/user-attachments/assets/95c83bed-a51f-4d5a-a0ac-3a7a9478fc04)

Даг будет запущен на них. Посмотрим, появится ли папка output_data

Послед этого добавим новый файл (как буд-то у нас появился update). В папке Output_data мы должны будет найти новый партиции.

# Первичный запуск дага

![image](https://github.com/user-attachments/assets/444f41a7-9837-479d-9c51-7e63c84d1338)

После некоторого дебага скрипт отработал

![image](https://github.com/user-attachments/assets/4b0e4cfe-c796-4a2d-9c8f-42c005a32b22)

Первый запуск был успешен, но там другой скрипт работа (из семинара). А вот последний - это уже целевой

Результат с очещенными данными залит на s3
Партиционированный по дате транзакции

![image](https://github.com/user-attachments/assets/bae87bc9-098d-4b21-9194-a1a617365685)

# второй запуск.

Добавляем еще один файл-период

![image](https://github.com/user-attachments/assets/0bb2173c-b901-4a40-8312-2167ee51b485)









